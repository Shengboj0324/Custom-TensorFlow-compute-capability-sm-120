# **Deep Analysis and Recommendations for the TensorFlow SM120 Build**

## **1. Upgrade and Align the Build Environment**

To target RTX 50-series GPUs and CUDA 12.8+, you must update the build environment to match the latest NVIDIA toolchain and dependencies. Currently, the project’s CI and Dockerfiles use CUDA 12.4 and cuDNN 9.7【8†L15-L22】, with compute capability “12.0” passed as a placeholder. This is insufficient for true sm_120 support, since CUDA 12.4 did **not** officially recognize compute capability 12.0 (as noted in your CMake config)【11†L60-L65】. Key upgrades and fixes include:

- **Base CUDA Toolkit:** Switch the Docker base image to one that provides **CUDA 12.8 (or newer)**. For example, use `nvidia/cuda:12.8.0-devel-ubuntu22.04` if available (or the closest latest version). Update environment variables in the Dockerfile accordingly (e.g. `TF_CUDA_VERSION=12.8`). This ensures the NVCC toolchain knows about `sm_120` architecture. In your current setup, you worked around missing support by targeting `sm_89` as the highest architecture【11†L60-L65】. With CUDA 12.8+, you can explicitly compile for **compute_120** to generate native code for RTX 50 GPUs.

- **cuDNN Library:** Upgrade to **cuDNN 9.8** (or the latest 9.x release that matches CUDA 12.8). Update the Docker installation step to pull the appropriate cuDNN packages or use NVIDIA’s installer if apt packages for 9.8 are not yet available. Also set `TF_CUDNN_VERSION` to “9.8” (or simply “9” if TensorFlow’s build scripts treat all 9.x similarly). This will align the build with the newer tensor core operations and ensure ABI compatibility with driver 570.x.

- **NVIDIA Driver Compatibility:** Ensure your host system (or wherever you deploy) has **NVIDIA driver 570.x** or newer. Drivers in the 570 series are required for CUDA 12.8 runtime support. Although this doesn’t directly affect the build process inside Docker, it’s critical for running the resulting package on your local machine. Double-check the driver’s compatibility with CUDA 12.8 using NVIDIA’s documentation.

- **LLVM/Clang Usage:** Re-evaluate the need for compiling with **LLVM 22** (Clang) versus NVCC. In your Dockerfile, `TF_CUDA_CLANG=1` and `CLANG_CUDA_COMPILER_PATH=/usr/bin/clang` are set, but simultaneously `--define=using_cuda_nvcc=true` was present in the Bazel config. This mix can cause confusion or unsupported configurations. If CUDA 12.8’s NVCC fully supports sm_120, the simplest path is to use NVCC (disable `TF_CUDA_CLANG`) for GPU code compilation. NVCC will produce correct PTX/Cubin for sm_120 once supported, and TensorFlow’s build will be more straightforward. If you **do** need LLVM 22 for experimental reasons (e.g. leveraging new clang CUDA features), ensure you use a matching LLVM toolchain in Docker (install LLVM 22 if available) and set Bazel to use it consistently (set `using_cuda_nvcc=false` and adjust Bazel defines accordingly). Also, incorporate any new compile flags needed for C++20/C++23 standards – you already added flags to suppress C23 extension warnings【51†L1-L9】【51†L13-L21】, which is good. Continue to monitor compilation output on new compilers and add `-Wno-*` suppressions or code adjustments as needed so that **no warnings turn into errors** (for example, new deprecation warnings or stricter checks in GCC/Clang should be handled by updating flags in `build_config.bzl` as you did for C23).

- **Bazel Version:** Use the latest **Bazel** that is supported by the TensorFlow version you build. Bazelisk 1.19.0 was used to auto-manage Bazel in the Dockerfile【14†L9-L17】. Check TensorFlow’s release notes for any bump in required Bazel version for building with newer CUDA – if so, update Bazelisk to a newer release. A too-old Bazel could mis-handle CUDA 12.8 or Python 3.13. Ensuring Bazel is up-to-date will also help with any build rule changes in newer TensorFlow code.

- **Python Version Compatibility:** Verify that the build process covers **Python 3.12 and 3.13** as targets. Your matrix already included up to 3.13【8†L15-L22】, which is great. For each Python version, make sure the ABI compatibility (e.g. manylinux compatibility) is maintained. If Python 3.13 is very new, you may need to update the manylinux container or pip wheel tagging in your build scripts so that the wheel is recognized for 3.13. Run the built wheel in a Python 3.13 environment to confirm there are no import issues (e.g. due to changes in the Python C-API or packaging).

In summary, **rebuild the Docker image with CUDA 12.8, cuDNN 9.8, and driver 570 support**, and propagate those version changes to all build configs. This addresses the root cause of sm_120 not being truly supported (you’ll now compile with actual sm_120 support rather than aliasing it to sm_89)【11†L60-L65】. It also prevents runtime incompatibilities that might occur if the wheel was built against older CUDA libraries. 

## **2. Fix Codebase Inconsistencies and Build Integration Issues**

Several code-level problems in the custom TensorFlow SM120 project need attention to achieve a clean, successful build:

- **Consolidate Duplicate or Outdated Source Files:** The repository contains overlapping source files for the custom ops (e.g. `sm120_ops.cc`, `sm120_tensorflow_ops.cc`, and a `sm120_complete_ops.cc` for batch-norm, etc.). It appears you intended to use “_fixed” versions of some files after applying fixes (for example, `sm120_ops_fixed.cc` and `sm120_optimized_kernels_fixed.cu`) – your BUILD rules reference these【57†lines 8-16】【57†lines 49-58】, but the actual files are missing in the codebase. This discrepancy will cause build failures (Bazel can’t find `sm120_ops_fixed.cc`). To resolve this: 

  - **Merge and Remove Redundant Ops Implementations:** Decide on one definitive implementation for each operation. For instance, if `sm120_ops.cc` and `sm120_tensorflow_ops.cc` were two versions, unify them into a single file. If you have already applied fixes to create a “fixed” variant, rename or replace the old file with the fixed one. Ensure the BUILD file references the correct filenames that actually exist. *Example:* If `sm120_ops.cc` is updated, either delete the old one and rename it to `sm120_ops_fixed.cc` (and commit that), or simply change the BUILD rule to use `sm120_ops.cc` again if it now contains all fixes. Consistency between the source tree and BUILD configuration is essential.

  - **Update Triton Support Namespace Consistently:** You patched XLA’s Triton support to avoid a naming collision by renaming the namespace to `triton_support_fixed` and the function to `set_matrix_3x3`【52†L6-L14】. This fix must be **propagated to any declarations or uses** of that function. If `set_matrix3x3` was only used internally in `triton_support.cc` (within XLA) and not declared in a header, you are safe. But if it was declared in `triton_support.h` under the old namespace, you should apply a similar patch to the header or, better, make the function `static` within the .cc file (since it’s purely internal utility). The goal is to prevent linker errors or ODR violations – as of now, you closed the original namespace and reopened a new one【52†L1-L9】【52†L15-L19】, which might leave other parts of XLA referencing a now-nonexistent `triton_support::set_matrix3x3`. Double-check XLA calls to this function. If any part of XLA (or tests) still expects the old name/namespace, they will fail. A more minimal fix could be to simply rename the function to `set_matrix_3x3` *within the same namespace*, if the collision was only on the function name. In any case, ensure header and source agree on naming after your changes.

  - **Template Static Assert Fixes:** You addressed template instantiation issues in `GpuLaunchConfig` and solver traits by splitting the `is_supported_type` logic【54†L1-L9】【55†L1-L7】. Make sure these fixes are correctly applied in *all* relevant places. For example, your patch snippet shows adding `is_supported_type_impl<T>()` and adjusting `static_assert` usage. Verify that similar patterns (templates with `static_assert`) in other files (like `cuda_solvers.h` or elsewhere in TensorFlow core) are handled. It’s worth compiling with **both GCC and Clang** to catch any template or constexpr issues – different compilers might instantiate templates at different times. The added flags `-Wno-error=c23-extensions` and `-Wno-error=unused-command-line-argument`【51†L1-L9】 already relax some compiler strictness, but logic errors must be solved in code. After applying your template fixes, the code should compile **without disabling `-Werror`** for those templates. If you still have to use `-Wno-error` flags extensively, consider if there’s an underlying C++ standard issue (e.g., using a newer C++20/23 feature not fully supported or an ABI mismatch). Use C++17 consistently for now (as set in CMakeLists) unless TensorFlow has moved to C++20 in the version you build – in that case, update the standard and verify all custom code adheres to it.

- **Linking the Custom GPU Ops Library:** A critical deployment issue is ensuring the custom ops (the `_sm120_ops.so` plugin) is correctly linked to TensorFlow’s runtime. Currently, you load this library at runtime via `tf.load_op_library` in `sm120_ops.py`. To avoid **undefined symbol errors** when loading:
  
  - Link against **TensorFlow’s framework library**. Typically, custom ops need to link to `libtensorflow_framework.so` (or the static equivalent) so that all TensorFlow API symbols (e.g. things like allocating tensors, stream executors, etc.) are resolved. In your `setup.py`, ensure that `tf_link_flags` includes `-ltensorflow_framework` and that `library_dirs` includes the path to TensorFlow’s libs (you are using `tf.sysconfig.get_lib()` which should provide this). This will embed a runtime dependency on TensorFlow’s symbols. At load time, if TensorFlow (the Python package) is already imported, its binaries should be loaded into the process and satisfy these symbols. If you still get unresolved symbols when calling `tf.load_op_library`, you might need to load with `RTLD_GLOBAL` or adjust how the extension is built. One robust approach is to **statically link** the necessary TensorFlow symbols into `_sm120_ops.so`. TensorFlow’s build sometimes provides a static archive of the framework. However, static linking can bloat the plugin and risk ABI mismatches – so dynamic linking to the TensorFlow framework (which is already loaded in the process) is usually better. Double-check that the plugin is being built **with the same ABI** (same `_GLIBCXX_USE_CXX11_ABI` setting) as the TensorFlow package. Mismatched C++ ABI could cause subtle errors or crashes. TensorFlow 2.10+ typically uses the C++11 ABI by default (ABI=1), but confirm via `tf.sysconfig.CXX11_ABI_FLAG`.

  - **Loading and Package Integration:** Your Python code uses `resource_loader.get_path_to_datafile("_sm120_ops.so")` to locate the .so, which assumes the .so is packaged in the Python module directory. Make sure your **wheel includes `_sm120_ops.so`** in the correct location. The `MANIFEST.in` looks correct【68†L1-L8】【68†L17-L19】 for including `.so` files under the `python/` package, and you call `setup.py` to build that extension. The likely issue is *when* this build happens. If the extension isn’t being built and packaged as part of the TensorFlow wheel, you need to adjust your build pipeline (see next section). If it is built, ensure that after installation, `sm120_ops.py` and `_sm120_ops.so` reside in the same package directory (e.g. `site-packages/tensorflow_sm120/python/`). Currently, `sm120_ops.py` is in a top-level `python` package (possibly meaning it installs as a module named `sm120_ops`). It might be cleaner to package these under a unique namespace (for example, as `tensorflow_sm120` package) to avoid any naming collision with other packages. Regardless, **test the installed wheel** on a clean environment: try importing `sm120_ops` and calling `sm120_ops.is_sm120_available()` to verify that the library loads without error. If it fails to load, use `ldd` on the `_sm120_ops.so` file to see what libraries it expects and adjust the linking accordingly (e.g., you might find it’s looking for a CUDA or TensorFlow .so in a path that isn’t available).

- **Bazel Build Integration vs. Setup.py:** You have two parallel build mechanisms – a Bazel BUILD file for the custom ops (defining `tf_cuda_library(sm120_...)` targets) and a `setup.py` using PyBind11. This redundancy can cause confusion. Decide on **one build system for the custom ops** to streamline deployment:

  - *Option A: Integrate fully into Bazel/TensorFlow build:* You can treat the custom ops as part of TensorFlow’s source and build them into the TensorFlow pip wheel. For example, you might add your `sm120_ops` libraries to the final linking of `tensorflow.python` or mark them as `ALWAYS_LINK` (you already did `alwayslink=1` on those targets【57†L49-L58】). To actually include them in the pip package, you might need to modify TensorFlow’s `build_pip_package` script or relevant Bazel rules so that the shared object gets packaged. TensorFlow’s pip wheel typically includes a monolithic `_pywrap_tensorflow_internal.so` that contains all built-in ops. If you prefer, you could merge your kernels into that library by adding your object files to the link of `pywrap_tensorflow_internal`. Alternatively, have Bazel produce a separate .so for SM120 ops and then script the packaging to include it. This approach is advanced and requires familiarity with TensorFlow’s build packaging, but it yields a **single unified wheel** (e.g., `tensorflow-2.x-sm120.whl`) that users can install to get everything at once. If you go this route, ensure that none of your custom symbols conflict with core TF symbols (unique op names and namespaces) and that registration happens at load. The `REGISTER_OP` and `REGISTER_KERNEL` calls in your C++ will run when the library is loaded (either as part of pywrap if linked in, or when tf.load_op_library is called if separate).

  - *Option B: Keep a separate plugin wheel:* Build TensorFlow itself unmodified (aside from small patches for sm_120 recognition) and build your SM120 ops as an independent add-on. In this scenario, you’d produce **two wheels** – e.g., `tensorflow_gpu-2.X.whl` (custom-built for CUDA 12.8) and `tensorflow_sm120-2.X.whl` for the ops. Users would install both. This can actually simplify testing (you can upgrade the ops independently of the TF version) and follows the model of other custom op packages (e.g., TensorFlow addons or custom kernels distributed separately). If you choose this, adjust your documentation and deployment guide to instruct installing both packages. Also, ensure the plugin’s `setup.py` enforces a dependency on the exact TensorFlow version it was built against (to avoid mismatch). For instance, use `install_requires=['tensorflow==2.xx']` in setup.py so that pip will refuse to install mismatched combinations. The downside is the user has to manage two packages, but the upside is modularity.

Given the complexity of integrating with the TensorFlow build, **Option B (separate plugin)** is often more maintainable. In that case, modify your Docker build pipeline to produce the plugin wheel as well: after building the TensorFlow wheel in Docker, install it in the container and run your `setup.py bdist_wheel` for the SM120 ops. You can then copy out both wheels. This way, you avoid trying to shoehorn the plugin build into Bazel, and you leverage the `setuptools` flow you already set up. Just be sure to test that the installed wheels work together.

- **Cleanup Build Scripts and Configs:** The provided scripts (like `comprehensive-build.sh` and GitHub workflows) should be updated once you make the above changes. Remove any now-unneeded workarounds. For example, if using CUDA 12.8 with NVCC, you may drop some of the `--expt-extended-lambda` or double `-gencode` entries in the BUILD file meant for older versions. Likewise, if you settle on one build method, remove references to the other (e.g., if Bazel is primary, maybe drop `setup.py` and vice versa). Ensuring the build scripts reflect the final approach will avoid confusion for future maintainers. Also double-check that the **CI config matches the Docker** – if you update Docker to Ubuntu 22.04 + CUDA 12.8, make sure any GitHub Actions workflow uses a compatible environment (or also builds inside Docker). Consistency across all build paths will prevent those “works locally, fails in CI” scenarios.

By fixing these code integration issues, you will eliminate persistent build errors like missing files and ensure that **all custom ops are correctly compiled and included** in the final artefacts. This step is critical to achieving a truly “deployable” package.

## **3. Ensure Hardware Compatibility and Runtime Stability**

Even after a successful build, you need to verify that the custom TensorFlow build runs correctly on the target hardware (and gracefully on non-target hardware). Some considerations:

- **Testing on RTX 50-series (sm_120) GPUs:** As soon as you have access to an RTX 5080/5090, run the full test suite on it. The device should be recognized by TensorFlow’s GPU device query. Your patched StreamExecutor code should now report compute capability 12.0 as supported (you added cases for cc_major==12) and label the device accordingly (e.g., adding “(sm_120)” to the device name)【22†L1-L4】. Check that this works: calling `tf.config.list_physical_devices('GPU')` should list the GPU without errors, and possibly `tf.test.is_gpu_available()` (for older TF) or similar calls won’t crash. Any failure to list the GPU or initialize it would indicate an issue in the CUDA driver compatibility or in the stream executor modifications. Given that your patch added `case 120: return true` in relevant places【22†L1-L4】,【22†L5-L10】, the TensorFlow runtime should treat compute capability 12.0 as a known-good device. Confirm that no fallback path is triggering (e.g., older TF might default to PTX JIT if unknown capability – with your patch it should load as if it were known).

- **Performance and Correctness on Pre-sm_120 GPUs:** Currently, you likely have only Ampere/Ada GPUs to test on (e.g., RTX 3090 or 4090). Your design includes fallback to “compatibility mode” when no 12.0 GPU is present【57†lines 113-120】. Ensure this fallback works correctly:
  
  - The environment variable `TF_CUDA_COMPUTE_CAPABILITIES` was set to `8.9` for the build, which means the compiled kernels target up to sm_89 code. This allows the binary to run on Ampere/Ada GPUs in the interim. Test your custom ops on a 30-series or 40-series GPU. The ops should load and execute (they will use the kernels you compiled for 8.6/8.9). Verify that `sm120_ops.is_sm120_available()` returns **False** on these older GPUs (since cc != 12.0), but the ops still function via fallback. From your example script, it looks like you call `advanced_matmul()` regardless of availability and only use `is_sm120_available()` to decide whether to run benchmarks or print warnings【60† (see `if not sm120_available: … continue using fallback`)】. Make sure that internally `advanced_matmul` does the right thing if `_SM120_AVAILABLE` is False – likely it calls `tf.matmul` as a fallback. If you haven’t implemented that fallback in code, consider doing so: the simplest approach is to wrap each custom op call such that if the custom kernel library isn’t loaded or the GPU isn’t of type sm_120, it just defers to standard TensorFlow ops. Your code hints at a `_config.fallback_to_standard` flag【88†L1-L9】 – ensure this is True by default. In testing, confirm that on a RTX 4090 you **get correct numerical results** for `sm120_ops.advanced_matmul`, either via the custom kernel (if you allow it) or via fallback.

  - **Optional:** You may actually *allow* the custom kernels to run on older GPUs if they are beneficial. Since you compile for sm_86 and sm_89 as well, your kernels could potentially accelerate operations even on Ampere. If that’s the case (e.g., your flash attention or fused kernels might still be faster than stock TF on a 4090), you might want to enable them. You could consider setting `is_sm120_available()` to true as long as the custom library is loaded **and** the GPU compute capability is >= 8.0 (or whatever minimum your kernels support). Alternatively, provide a config switch to force-enable SM120 ops on lower architectures for testing. This flexibility can help you gather performance data on current hardware and ensure the kernels are truly optimized. Just document clearly that on non-50 series GPUs, these ops run in “compatibility mode” and may not give the full 5th-gen Tensor Core speedups, but should still function. 

- **Memory and Resource Checks:** Running on a real RTX 50, pay attention to GPU memory usage and initialization. New architectures sometimes have quirks (e.g., different memory alignment requirements or stream priorities). Test scenarios with multiple GPUs as well, if applicable, to ensure your device selection logic doesn’t break (for instance, `get_sm120_device_info()` should handle multiple GPUs, some sm_120, some not). Also test that if **no GPU is present**, the TensorFlow build still loads and runs CPU-only code without trying to access null pointers (your genrule writes a warning in that case【57†lines 113-120】, but also the Python code should handle `_SM120_AVAILABLE=False` gracefully). In other words, verify that the custom build remains usable in environments without an RTX 50 GPU (it should behave like normal TensorFlow CPU).

- **Stability and Error Handling:** Push the system by running long training or integration tests. The custom ops should not leak memory or cause illegal memory accesses. Since you wrote custom CUDA kernels (e.g. in `sm120_optimized_kernels.cu`), it’s crucial to test them thoroughly. Use TensorFlow’s debugging tools: enable GPU memory growth and see if any OOM occurs unexpectedly when using your ops, and run with `cuda-memcheck` if possible to catch memory errors in kernels. Also try mixing your ops with standard ops in a graph to ensure interoperability. Any persistent errors like **illegal instruction** or **cudaError misaligned address** would suggest an issue in the kernel code that needs fixing for the new architecture. Given that RTX 50 may introduce new hardware features (e.g., new tensor core instructions or increased register file size), ensure that your launch configurations (`GpuLaunchConfig` etc.) are still valid. You set a max register count of 128 in compilation flags【11†L61-L65】 – on newer GPUs, you might experiment with higher or let the compiler decide. Keep an eye on the compiler PTXAS output (`-Xptxas=-v` flag is already set) to see register usage and occupancy. Optimize these if needed for sm_120.

- **Compatibility with Future TensorFlow Releases:** As TensorFlow evolves, some of your patches might become obsolete or even conflict with upstream changes. For instance, by the time official TensorFlow supports CUDA 12.8 and RTX 50, they will likely add their own `case 12` in the device code, or handle the Triton issue differently. Plan to **rebase your custom build on newer TensorFlow releases periodically**. If TensorFlow 2.15+ includes partial support for compute capability 9.0 (Hopper) or future Blackwell architectures, adopt their approach. Upstream may also fix the template issues (TensorFlow’s maintainers often adjust code to support newer compilers as they come out). When such fixes appear, you can drop your local patch to avoid redundancy. In practical terms, monitor the TensorFlow repository for commits related to CUDA version bumps or new device support. Upgrading your base code can also bring performance improvements and bug fixes that benefit your project. Just be prepared to re-apply or adjust your custom kernel integration when moving to a new version.

In summary, treat this phase as a **validation and hardening** step. You want to ensure that your “production-ready” claims hold true: no crashes, correct results, and performance gains on the real hardware. It’s great that you have a comprehensive test suite; make sure to run it in various environments (50-series GPU, older GPU, CPU-only) and get zero failures. Any test failures or persistent warnings should be investigated – they often point to edge cases (e.g., numerical stability in flash attention or minor differences in floating-point results). Tweak tolerances in tests if needed, but also confirm there isn’t a deeper issue (like an uninitialized buffer leading to random output differences). Achieving a **clean bill of health in all tests** will give confidence that the custom build is truly robust.

## **4. Documentation and Next Steps**

With the technical issues addressed, update your documentation to reflect the changes and guide users (and future developers) effectively:

- **Deployment Guide:** Revise the deployment instructions to match the new process. If you consolidate into one wheel, document the single install step. If you provide two wheels (TensorFlow core and SM120 ops), explicitly instruct users to install both (e.g., “`pip install tensorflow_gpu...whl tensorflow_sm120...whl`”). Also mention the requirement of CUDA 12.8 and drivers 570 – your checklist in the Deployment Guide should list the exact versions now (currently it generically says “CUDA 12.8+” and “cuDNN 9.7-9.8”【44†L1-L9】; you can firm that up to “CUDA 12.8 and cuDNN 9.8 required”). Remove any outdated reference to 12.4 or older environments.

- **README and Project Status:** Adjust the README badges or text if needed (for example, if now targeting TensorFlow 2.15 or 2.20 specifically, ensure the badge reflects the correct version). It currently mentions “TensorFlow 2.10+” which is quite broad【37†L1-L9】 – after your updates you might specify the tested compatible range (e.g., “based on TensorFlow 2.15 with SM120 enhancements”). The Project Status and final summary docs that claimed 100% completion can be updated to note the recent fixes. Since you discovered persistent issues, it’s worth adding an addendum: e.g., “All critical build and runtime issues have now been resolved as of <date>, after updating the toolchain and codebase.” This is both to inform users and to keep a historical log for yourself of what changed.

- **Guidance for Future Contributions:** Given the complexity of this custom build, it may be useful to document how one would update the build for future CUDA versions or new GPU architectures. You have essentially created a template for adding new device support to TensorFlow. Consider adding a short section in the documentation for developers, explaining the key places to change when a new compute capability comes out (stream executor, XLA device compiler mapping, etc.). This can be gleaned from the changes you made (like which files were patched for sm_120 support). For instance: “*To add support for a new CUDA architecture, update `cuda_gpu_executor.cc` and `cuda_gpu_driver.cc` to recognize the compute capability, update XLA’s `nvptx_compiler.cc` to target the new sm_xy PTX, and ensure Bazel is passing the correct `-gencode` flags.*” Having these notes means your project can more easily adapt to, say, an RTX 60-series in the future.

- **Community and Upstream Collaboration:** If possible, engage with the TensorFlow community about these changes. By 2025, others will be interested in RTX 50 support too. Consider contributing back some of the generic fixes: for example, the template instantiation fixes and warning suppressions for C++23 could be proposed upstream, as they benefit TensorFlow on modern compilers. Upstreaming what you can reduces the maintenance burden on your fork. For the proprietary or experimental parts (like your custom kernels and flash attention), you might keep those in your project, but still pay attention to whether TensorFlow or NVIDIA release their own implementations of similar functionality. (For example, if future TF includes its own flash attention kernel, you’ll want to measure yours against it and possibly toggle which to use.)

- **Continuous Integration and Regression Prevention:** Now that the build is working, set up CI jobs to **build and test the package regularly** (if you haven’t already). This will catch any new issues early. For example, run a nightly build against the latest TensorFlow repository (if you plan to track upstream) or at least against your stable branch with the latest compilers. Include tests on multiple configurations (perhaps using environment matrix: one with an emulated or smaller GPU for basic tests, one without GPU to test CPU-only import, etc.). Your GitHub Actions workflow already defines multiple jobs for different Python versions and CUDA architectures【8†L15-L22】 – be sure to update those to reflect sm_120 (once supported) and possibly add a job that actually runs the ops (though in CI you might not have a 50-series GPU; you can still run the ops on CPU or a lower GPU to ensure the code doesn’t crash when it falls back). The goal is to maintain a **“zero tolerance” for build failures**, as your documentation proudly noted.

By implementing these recommendations, you will transform your custom TensorFlow build into a truly production-grade solution for RTX 50-series GPUs. The **key outcomes** will be: a successful build with CUDA 12.8 and cuDNN 9.8, a clean integration of all custom kernels, no missing symbols or files, full compatibility with both new and old GPUs, and thorough documentation to support users. This comprehensive overhaul addresses the persistent problems you faced (Docker build failures, undeployable artifacts, code bugs) and sets up the project for long-term success. 

With these fixes in place, you should have an **“absolutely successful” SM120 TensorFlow build** – one that installs smoothly, recognizes RTX 5090/5080 GPUs out-of-the-box, and delivers the promised performance benefits with stability. Good luck, and happy training on your RTX 50-series GPU!
